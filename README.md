# Building-a-deep-neural-network-from-scratch
Neural Network from Scratch

This repository provides an in-depth exploration of building deep neural networks from scratch. Unlike using pre-existing frameworks, this project focuses on implementing neural networks mathematically, leveraging concepts such as gradient descent and partial derivatives in backpropagation.

The process starts with the initialization of random weights and biases, which are crucial for learning. Then, forward propagation is performed, where inputs are passed through the network, activations are computed, and predictions are generated.

The next step is backpropagation, where errors are calculated and propagated backwards through the network. This involves the calculation of the activation function's logarithmic loss and the utilization of gradient descent to update the weights and biases iteratively.

To utilize the trained network, a prediction function is implemented, allowing for predictions on new data. Finally, all the components are encapsulated within a function called "neural_network," enabling easy usage and experimentation.

By understanding the fundamental mathematics behind neural networks, this project provides a solid foundation for building and customizing deep learning models. It serves as a valuable resource for those interested in comprehending the inner workings of neural networks and optimizing their performance.

In this repository, you will find that all the codes are written in a verbose style, emphasizing the clarity and understanding of the underlying logic. Each step is thoroughly explained through comments, ensuring that the code is self-documenting and facilitating a deeper comprehension of the implementation details. This approach aims to provide you with a clear understanding of the code's functionality, making it easier for you to navigate, modify, and build upon the existing codebase.
