{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a32f9c8",
   "metadata": {},
   "source": [
    "## Building Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d45f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs , make_circles\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd87b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_architecture(X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Asks for user input to define the architecture of a deep neural network.\n",
    "    \n",
    "    Args:\n",
    "    X (ndarray): The input data of shape (input_size, m), where input_size is the number of features\n",
    "                     and m is the number of training examples.\n",
    "    y (ndarray): The target labels of shape (output_size, m), where output_size is the number of classes\n",
    "                     and m is the number of training examples.\n",
    "    \n",
    "    Returns:\n",
    "    dimensions (list): A list containing the dimensions of the arrays for each hidden layer in the network.\n",
    "    Each element in the list represents the number of neurons in the corresponding layer.\n",
    "    The first element represents the number of input features in the input layer, followed by\n",
    "    the number of neurons in each hidden layer.\n",
    "    \n",
    "    Note:\n",
    "    This function prompts the user to enter the desired number of layers and the number of neurons in each layer\n",
    "    (excluding the input and output layer). The function then returns a list of dimensions, which can be used\n",
    "    to define the architecture of the deep neural network. It will also be used later in the init_dl function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of total layers excluding input  and output layer\n",
    "    \n",
    "    nb_layers = int(input(\"enter the number of hidden layers you desire to put in your deep neural network\"))\n",
    "    \n",
    "    # dimensions array is to store the number of neurons of each layer also number of input features\n",
    "    \n",
    "    dimensions =[]\n",
    "    \n",
    "    # for loop on hidden layers only\n",
    "    \n",
    "    for i in range(nb_layers) :\n",
    "            \n",
    "       # for hidden layers\n",
    " \n",
    "            nb_hidden_layers = int(input(\"enter the number of lneurals you desire to put in the {} th layer\".format(i+1)))\n",
    "            dimensions.append(nb_hidden_layers)\n",
    "            \n",
    "            \n",
    "       # wih each iteration and in each case we append the dimensions list respectively to keep the order of layers\n",
    "    \n",
    "        \n",
    "    # insert the input layer in position 0\n",
    "    \n",
    "    dimensions.insert(0, X.shape[0])\n",
    "    \n",
    "    # append the output layer to dimensions\n",
    "    \n",
    "    dimensions.append(y.shape[0])\n",
    "    \n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe922078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10, 50, 120, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af88444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dl(dimensions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializes the parameters of a deep neural network based on the given dimensions.\n",
    "    \n",
    "    Args:\n",
    "    dimensions (list): A list containing the dimensions of the arrays for each layer in the network.\n",
    "    Each element in the list represents the number of neurons in the corresponding layer.\n",
    "    For example, for a network with n layers, the dimensions list will have n+2 elements,\n",
    "    where the first element is the input dimension and the last element is the output dimension.\n",
    "    \n",
    "    Returns:\n",
    "    parameters (dict): A dictionary containing the initialized parameters of the network.\n",
    "    The keys of the dictionary represent the layer order, and the values are the initialized\n",
    "    weight matrices and bias vectors for each layer. The shape of the weight matrices will\n",
    "    be R(ni x n(i-1)), where ni is the number of neurons in the current layer and n(i-1) is\n",
    "    the number of neurons in the previous layer.\n",
    "    \n",
    "    Note:\n",
    "    The number of parameters in the network can be deduced from the number of layers (n) as 2*n.\n",
    "    \"\"\"\n",
    "    \n",
    "    # we initialize N as the len of dimensions\n",
    "    \n",
    "    N = len(dimensions)\n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1,N): \n",
    "        \n",
    "        # storing value Wi in a dictionnary with shape(ith layer,\"i-1\"th layer)\n",
    "        # dimensions(i) represent the number of neural in current layer and dimensions(i-1) number\n",
    "        # of neural in previous layer. NB: if i=1 dimensions(i-1) represents number of input features \n",
    "        \n",
    "        parameters[\"W\"+str(i)] = np.random.randn(dimensions[i],dimensions[i-1])\n",
    "        \n",
    "        # same logic is applicable to the biais that is in all cases a vector\n",
    "        \n",
    "        parameters[\"b\"+str(i)] = np.random.randn(dimensions[i],1)\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# we must not forget that we will apply a transpose to our Wi matrix later\n",
    "# like what we did in 2layers code, in order to make the dot product possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b2d662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.57449896, -0.43171238],\n",
       "        [ 1.4988324 ,  1.65714466],\n",
       "        [ 0.51954155,  0.95288559],\n",
       "        [ 0.29348592, -0.25928915],\n",
       "        [ 1.08699871, -2.0237977 ],\n",
       "        [-0.05732422,  0.00356909],\n",
       "        [ 0.19084422, -0.74684245]]),\n",
       " 'b1': array([[ 0.79826431],\n",
       "        [-0.21454619],\n",
       "        [-0.29454813],\n",
       "        [-0.32018399],\n",
       "        [-1.37081402],\n",
       "        [ 0.22499986],\n",
       "        [ 0.68687855]]),\n",
       " 'W2': array([[ 1.08849617, -0.79250705, -0.04993504,  0.75249346,  1.91101971,\n",
       "         -1.0667166 , -0.3598867 ],\n",
       "        [-1.44825005, -1.72221734,  1.05715987, -0.68294562,  0.70377928,\n",
       "         -0.32246466,  0.02225763],\n",
       "        [-1.25000813, -0.35274934, -0.83905715, -1.00179711, -0.31568369,\n",
       "          1.21416963,  0.14865031],\n",
       "        [ 2.17334999, -0.88144988,  0.10827437,  0.60495658,  0.58876779,\n",
       "         -0.23790771, -0.91332559],\n",
       "        [ 2.22945409, -0.31568298,  1.35472489,  0.7810959 ,  0.79887496,\n",
       "          0.10940601,  0.73532978],\n",
       "        [ 0.98677662,  0.33728608,  1.91385261,  0.95065599,  0.31331134,\n",
       "         -1.84262914, -1.2163135 ],\n",
       "        [-0.79785562,  0.22051337, -1.71698514,  0.55490154, -1.36827702,\n",
       "          1.13327455,  1.56283311]]),\n",
       " 'b2': array([[ 0.23874431],\n",
       "        [-1.51090537],\n",
       "        [ 2.02949363],\n",
       "        [-0.62566797],\n",
       "        [-0.3605262 ],\n",
       "        [-0.3698668 ],\n",
       "        [ 0.07239987]]),\n",
       " 'W3': array([[ 0.54967455,  1.66324321, -0.65426407, -0.56848864, -1.23340346,\n",
       "          0.43636628, -0.69743417],\n",
       "        [ 0.61617188,  0.88145624, -0.13100693,  0.69186128,  0.09415495,\n",
       "         -0.44670769,  0.94874173],\n",
       "        [-0.12038495,  1.02095517,  0.21222274,  0.43084103,  0.00612818,\n",
       "          0.29234796, -0.87353909]]),\n",
       " 'b3': array([[ 0.29695137],\n",
       "        [-0.19459832],\n",
       "        [-1.27537596]]),\n",
       " 'W4': array([[-0.492861  ,  0.76397268, -0.1093641 ],\n",
       "        [-1.45627303,  0.99543986, -0.77478014]]),\n",
       " 'b4': array([[-1.18613937],\n",
       "        [ 0.5578326 ]]),\n",
       " 'W5': array([[ 0.33695785, -0.02646221]]),\n",
       " 'b5': array([[0.34265849]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "410f4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs forward propagation in a deep neural network to compute the activations of each layer.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input data of shape (input_dim, m), where input_dim is the number of input features\n",
    "            and m is the number of examples in the batch.\n",
    "        parameters (dict): A dictionary containing the parameters of the neural network.\n",
    "            The keys of the dictionary represent the layer order, and the values are dictionaries\n",
    "            containing the weight matrices (keys: \"W1\", \"W2\", ..., \"WN\") and bias vectors\n",
    "            (keys: \"b1\", \"b2\", ..., \"bN\") for each layer.\n",
    "\n",
    "    Returns:\n",
    "        activations (dict): A dictionary containing the activations of each layer in the network.\n",
    "            The keys of the dictionary represent the layer order, and the values are the activation\n",
    "            arrays (A1, A2, ..., AN) for each layer. The shape of each activation array is\n",
    "            (n_i, m), where n_i is the number of neurons in the corresponding layer and m is the\n",
    "            number of examples in the batch.\n",
    "\n",
    "    Note:\n",
    "        This function iterates over the layers of the neural network, starting from the first hidden layer (index 1)\n",
    "        to the output layer (index N). For each layer, it computes the weighted sum (Z) of the previous layer's\n",
    "        activations and the corresponding weight matrix, adds the bias vector, and applies the sigmoid activation\n",
    "        function. The resulting activations are stored in the `activations` dictionary and returned as the output.\n",
    "    \"\"\"\n",
    "    \n",
    "    # first we initialize a dictionnary that will store activations Ai\n",
    "    # We will also give this dictionnary a key A0 with value X to solve the first layer problem where Z1 = W1.X + b1\n",
    "    \n",
    "    activations = { \"A0\" : X }\n",
    "    \n",
    "    # N-1 is the total number of layers that we will obtain using integer division\n",
    "    \n",
    "    N = len(parameters)//2\n",
    "    \n",
    "    # this loop will start from 1 since we have already determined W0 to be X and go to N\n",
    "    \n",
    "    for i in range(1,N+1) :\n",
    "        \n",
    "        # Each loop will compute a function Zi , and the next loop will overwrite the previous Zi to compute Z(i+1)\n",
    "        \n",
    "        Z = parameters[\"W\"+str(i)].dot(activations[\"A\"+str(i-1)]) + parameters[\"b\"+str(i)]\n",
    "        \n",
    "        # Here we store the activation output using sigmoid function in the activations dict\n",
    "        \n",
    "        activations[\"A\"+str(i)] = 1 + (1+np.exp(-Z))\n",
    "        \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "221c23a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31119/2620156903.py:47: RuntimeWarning: overflow encountered in exp\n",
      "  activations[\"A\"+str(i)] = 1 + (1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "c = forward_propagation(X,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9fd685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation (y, activations, parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs backpropagation in a deep neural network to compute the gradients of the parameters.\n",
    "\n",
    "    Args:\n",
    "        y (array-like): The true labels or target values of shape (1, m), where m is the number of examples in the batch.\n",
    "        activations (dict): A dictionary containing the activations of each layer in the network.\n",
    "                            The keys of the dictionary represent the layer order, and the values are the activation\n",
    "                            arrays (A1, A2, ..., AN) for each layer.\n",
    "        parameters (dict): A dictionary containing the parameters of the neural network.\n",
    "                           The keys of the dictionary represent the layer order, and the values are dictionaries\n",
    "                           containing the weight matrices (keys: \"W1\", \"W2\", ..., \"WN\") and bias vectors\n",
    "                           (keys: \"b1\", \"b2\", ..., \"bN\") for each layer.\n",
    "\n",
    "    Returns:\n",
    "        gradients (dict): A dictionary containing the gradients of the parameters.\n",
    "                          The keys of the dictionary represent the layer order, and the values are the gradients\n",
    "                          of the weight matrices (dW1, dW2, ..., dWN) and bias vectors (db1, db2, ..., dbN) for each layer.\n",
    "\n",
    "    Note:\n",
    "        This function performs backpropagation by iterating through the layers of the neural network in reverse order.\n",
    "        It starts with the last layer and computes the gradients of the parameters using the generalized formulas\n",
    "        obtained from the two-layer neural network case. The partial derivatives are specific to the sigmoid activation\n",
    "        function and the log loss function derived from the Bernoulli probability law. The gradients are then stored\n",
    "        in the `gradients` dictionary and returned as the output.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing the an empty dictionnary gradients\n",
    "    \n",
    "    gradients = {}\n",
    "    \n",
    "    # this line will help us get the dimensions of the output vector, in other terms size of the data we trained\n",
    "    # we do it to get 1/m, the scaling factor \"1/m\" is used to normalize the average gradient, not the individual \n",
    "    # gradient updates, and it helps to ensure consistent and stable learning across different dataset sizes. \n",
    "    # The actual scaling of the gradient update step is done by the learning rate.\n",
    "    \n",
    "    m = y.shape[1] \n",
    "    \n",
    "    N = len(parameters)//2\n",
    "    \n",
    "    # this line here is used to initialize the partial derivative of the last layer of our neural network \n",
    "    # wich is in fact the first step of our back propagation since we go in reversed path to generate partial derivatives\n",
    "    \n",
    "    dZ = activations[\"A\"+str(N)] - y\n",
    "    \n",
    "    # Proceeding with reversed for loop in order to go throught the NN from last to first layer\n",
    "    \n",
    "    for i in reversed(range(1,N+1)) :\n",
    "        \n",
    "        # we intialize A to represent A(i-1), because we noticed that apart from dZN that represents the last layer\n",
    "        # all of the other equations will use only A(i-1)\n",
    "        \n",
    "        A = activations[\"A\" + str(i-1)]\n",
    "        \n",
    "        # we generate W to represent Wi because we will use it to compute dZ(i-1)\n",
    "        \n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        \n",
    "        # we apply the formulas we reached from generalizing the computations we got previously on the 2 layers NN\n",
    "        \n",
    "        gradients[\"dW\" + str(i)] =  1/m * np.dot(dZ,A.T)\n",
    "        \n",
    "        # same with biais formula we apply the generalized form we got from the 2 layers NN\n",
    "        \n",
    "        gradients[\"db\" + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        # this line is to update the value of dZ in order to jump to next layer wich is in fact and order of forward propagation\n",
    "        # the precedent layer of the actual layer. Each iteration new dZ will overwrite the old dZ\n",
    "        # and we must not forget to apply the condicion on i > 1 beacuse dZ0 doesnt exist, because in iteration i we\n",
    "        # compute dZ(i-1)\n",
    "        \n",
    "        if i > 0 :\n",
    "            dZ =  np.dot(W.T,dZ) * A * (1-A)    \n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9995d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW4': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan]]),\n",
       " 'db4': array([[nan]]),\n",
       " 'dW3': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'db3': array([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]]),\n",
       " 'dW2': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'db2': array([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]]),\n",
       " 'dW1': array([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]]),\n",
       " 'db1': array([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed34b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_func(A, y):\n",
    "    \"\"\"\n",
    "    Compute the logistic loss function for binary classification.\n",
    "\n",
    "    The logistic loss function, also known as the binary cross-entropy loss, is a common\n",
    "    cost function used in binary classification tasks. It measures the discrepancy between\n",
    "    the predicted probabilities (A) and the true labels (y) for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy.ndarray): Predicted probabilities of shape (m,) where m is the number of samples.\n",
    "    y (numpy.ndarray): True labels of shape (m,) where m is the number of samples.\n",
    "\n",
    "    Returns:\n",
    "    float: The computed logistic loss.\n",
    "\n",
    "    Explanation:\n",
    "    The logistic loss function is derived from the concept of maximum likelihood estimation\n",
    "    for binary outcomes. By taking the negative log-likelihood of the predicted probabilities\n",
    "    (A) for the positive class (y=1) and the negative class (y=0), and averaging it across\n",
    "    all samples, we obtain a measure of the model's performance.\n",
    "\n",
    "    To avoid vanishing probabilities and numerical instability during computation, the loss\n",
    "    function is constructed using logarithms. The logarithm helps to compress the range of\n",
    "    probabilities and prevents multiplication of very small values, which could lead to\n",
    "    underflow issues. By taking the negative logarithm of the predicted probabilities, we\n",
    "    penalize large deviations from the true labels and encourage the model to better fit the\n",
    "    data.\n",
    "\n",
    "    The logistic loss function is commonly used as the cost function in logistic regression\n",
    "    and as the final layer's activation function in binary classification neural networks.\n",
    "    It provides a continuous, differentiable, and interpretable measure of the model's\n",
    "    performance, allowing for efficient optimization through techniques like gradient\n",
    "    descent.\n",
    "\n",
    "    References:\n",
    "    - Logistic regression: https://en.wikipedia.org/wiki/Logistic_regression\n",
    "    - Binary cross-entropy loss: https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
    "    \"\"\"\n",
    "    return (1 / len(y)) * np.sum(-y * np.log(A) - (1 - y) * np.log(1 - A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea811d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(gradients, parameters, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates the weights and biases of a deep neural network based on the computed gradients.\n",
    "\n",
    "    Args:\n",
    "        gradients (dict): A dictionary containing the gradients of the parameters.\n",
    "                            The keys of the dictionary represent the layer order, and the values are the gradients\n",
    "                            of the weight matrices (dW1, dW2, ..., dWN) and bias vectors (db1, db2, ..., dbN) for each layer.\n",
    "        parameters (dict): A dictionary containing the parameters of the neural network.\n",
    "                            The keys of the dictionary represent the layer order, and the values are dictionaries\n",
    "                            containing the weight matrices (keys: \"W1\", \"W2\", ..., \"WN\") and bias vectors\n",
    "                            (keys: \"b1\", \"b2\", ..., \"bN\") for each layer.\n",
    "        learning_rate (float): The learning rate or step size used for updating the parameters.\n",
    "\n",
    "    Returns:\n",
    "        parameters (dict): A dictionary containing the updated parameters of the neural network.\n",
    "                            The keys of the dictionary represent the layer order, and the values are dictionaries\n",
    "                            containing the updated weight matrices and bias vectors for each layer.\n",
    "\n",
    "    Note:\n",
    "        This function updates the weights and biases of the neural network using the computed gradients.\n",
    "        It iterates through each layer of the network and performs the parameter updates based on the learning rate\n",
    "        and corresponding gradient values. The updated parameters are stored in the `parameters` dictionary and\n",
    "        returned as the output.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(parameters)//2\n",
    "    \n",
    "    # again a for loop to update every weight and bias in the whole network respectively to his gradients\n",
    "    \n",
    "    for i in range(1,N+1) :\n",
    "        \n",
    "        # update weight Wi following gradient dWi\n",
    "        \n",
    "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - learning_rate * gradients[\"dW\" + str(i)]\n",
    "        \n",
    "        # update bias bi following gradient dbi\n",
    "        \n",
    "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - learning_rate * gradients[\"db\" + str(i)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83554d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(X, y, parameters, ax):\n",
    "    \"\"\"\n",
    "    Visualizes the training progress of the deep neural network.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): The input data of shape (input_size, m), where input_size is the number of features\n",
    "                     and m is the number of training examples.\n",
    "        y (ndarray): The target labels of shape (output_size, m), where output_size is the number of classes\n",
    "                     and m is the number of training examples.\n",
    "        parameters (dict): A dictionary containing the trained parameters of the neural network.\n",
    "                           The keys of the dictionary represent the layer order, and the values are dictionaries\n",
    "                           containing the weight matrices and bias vectors for each layer.\n",
    "        ax (AxesSubplot): The axes to plot the visualization.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform forward propagation to get the final activations\n",
    "    activations = forward_propagation(X, parameters)\n",
    "    A_final = activations['A' + str(len(parameters) // 2)]\n",
    "\n",
    "    # Scatter plot of the input data\n",
    "    ax[2].scatter(X[0, :], X[1, :], c=y.flatten(), cmap='coolwarm', edgecolors='k')\n",
    "    ax[2].set_xlabel('X1')\n",
    "    ax[2].set_ylabel('X2')\n",
    "    ax[2].set_title('Input Data')\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    x1_min, x1_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    x2_min, x2_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1), np.arange(x2_min, x2_max, 0.1))\n",
    "    input_data = np.vstack((xx1.ravel(), xx2.ravel()))\n",
    "    Z = predict(input_data, parameters)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    ax[2].contourf(xx1, xx2, Z, alpha=0.4, cmap='coolwarm')\n",
    "    ax[2].set_xlim(xx1.min(), xx1.max())\n",
    "    ax[2].set_ylim(xx2.min(), xx2.max())\n",
    "    ax[2].set_title('Decision Boundary')\n",
    "\n",
    "    # Plot the loss curve\n",
    "    ax[0].plot(loss_list, label=\"Log Loss training curve\")\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    ax[0].set_ylabel('Log Loss')\n",
    "    ax[0].set_title('Training Loss')\n",
    "\n",
    "    # Plot the accuracy curve\n",
    "    ax[1].plot(acc_list, label=\"Accuracy training curve\")\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Training Accuracy')\n",
    "\n",
    "    # Show legend for all plots\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99952788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_network(X, y, learning_rate = 0.001, n_iter = 1000):\n",
    "    \"\"\"\n",
    "    Trains a deep neural network using forward propagation, backpropagation, and gradient descent.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): The input data of shape (input_size, m), where input_size is the number of features\n",
    "                     and m is the number of training examples.\n",
    "        y (ndarray): The target labels of shape (output_size, m), where output_size is the number of classes\n",
    "                     and m is the number of training examples.\n",
    "        learning_rate (float, optional): The learning rate or step size used for updating the parameters during gradient descent.\n",
    "                                          Defaults to 0.001.\n",
    "        n_iter (int, optional): The number of iterations or epochs to train the neural network. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        parameters (dict): A dictionary containing the trained parameters of the neural network.\n",
    "                           The keys of the dictionary represent the layer order, and the values are dictionaries\n",
    "                           containing the weight matrices and bias vectors for each layer.\n",
    "\n",
    "    Note:\n",
    "        This function trains a deep neural network by performing forward propagation, backpropagation, and gradient descent.\n",
    "        It initializes the parameters, performs the specified number of iterations, and updates the parameters\n",
    "        in each iteration based on the computed gradients. The training progress is visualized by plotting the\n",
    "        log loss and accuracy curves over iterations.\n",
    "\n",
    "        The function returns the trained parameters that can be used for making predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # initializing the parameters\n",
    "    \n",
    "    dimensions = dnn_architecture()\n",
    "    \n",
    "    # generate parameters using initialization function\n",
    "    \n",
    "    parameters = init_dl(X, y, dimensions)\n",
    "    \n",
    "    # initialize two empty lists that will store values of loss function and accuracy\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    # gradient descent algorithm where in each iteration we do a forwad prop, a back prop and an update of the parameters\n",
    "    # tqdm is a library that put a progress bar on the progression , it's derived from arabic \"taqadom\" wich means progressoin\n",
    "    \n",
    "    for i in tqdm(range(n_iter)):\n",
    "\n",
    "        activations = forward_propagation(X, parameters)\n",
    "        gradients = back_propagation(y, parameters, activations)\n",
    "        parameters = update(gradients, parameters, learning_rate)\n",
    "        \n",
    "        # compute the final value of output layer in activations so we use it later in comparison of log_loss \n",
    "        \n",
    "        N = len(parameters)//2\n",
    "        A_final = activations['A' + str(N)]\n",
    "        \n",
    "        # each ten iterations we will compute the log_loss between y and activations of output layer and in parallel\n",
    "        # we also analyze the accuracy between predicted values and y\n",
    "        \n",
    "        if i%10 == 0 :\n",
    "            \n",
    "            # compute log_loss to see the difference of activations from labeled dataset\n",
    "            \n",
    "            loss_list.append(log_loss(y,A_final))\n",
    "            y_pred = predict(X,parameters)\n",
    "            \n",
    "            # compute the current accuracy of the model\n",
    "            \n",
    "            accuracy = accuracy_score(y.flatten(),y_pred.flatten())\n",
    "            acc_list.append(accuracy)\n",
    "            \n",
    "\n",
    "    # plotting training curves\n",
    "    \n",
    "    fig , ax = plt.subplots(nrows=1 , ncols=3 , figsize=(18,4))\n",
    "    ax[0].plot(loss_list, label=\"Log Loss training curve\")\n",
    "    ax[0].legend\n",
    "    \n",
    "    ax[1].plot(acc_list, label=\"Accuracy training curve\")\n",
    "    ax[1].legend\n",
    "    \n",
    "    visualization(X, y, parameters, ax)\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138aa4c0",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28134acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions de X: (2, 100)\n",
      "dimensions de y: (1, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we generate the dataset to test our Deep Neural Network \n",
    "\n",
    "X, y = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=0)\n",
    "X = X.T\n",
    "y = y.reshape((1, y.shape[0]))\n",
    "\n",
    "print('dimensions de X:', X.shape)\n",
    "print('dimensions de y:', y.shape)\n",
    "\n",
    "plt.scatter(X[0, :], X[1, :], c=y, cmap='summer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9968e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the number of hidden layers you desire to put in your deep neural network3\n",
      "enter the number of lneurals you desire to put in the 1 th layer32\n",
      "enter the number of lneurals you desire to put in the 2 th layer64\n",
      "enter the number of lneurals you desire to put in the 3 th layer32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_31119/2620156903.py:47: RuntimeWarning: overflow encountered in exp\n",
      "  activations[\"A\"+str(i)] = 1 + (1+np.exp(-Z))\n",
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdeep_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 50\u001b[0m, in \u001b[0;36mdeep_neural_network\u001b[0;34m(X, y, learning_rate, n_iter)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_iter)):\n\u001b[1;32m     49\u001b[0m     activations \u001b[38;5;241m=\u001b[39m forward_propagation(X, parameters)\n\u001b[0;32m---> 50\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m update(gradients, parameters, learning_rate)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# compute the final value of output layer in activations so we use it later in comparison of log_loss \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 42\u001b[0m, in \u001b[0;36mback_propagation\u001b[0;34m(y, activations, parameters)\u001b[0m\n\u001b[1;32m     37\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameters)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# this line here is used to initialize the partial derivative of the last layer of our neural network \u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# wich is in fact the first step of our back propagation since we go in reversed path to generate partial derivatives\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m dZ \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m y\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Proceeding with reversed for loop in order to go throught the NN from last to first layer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,N\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)) :\n\u001b[1;32m     47\u001b[0m     \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# we intialize A to represent A(i-1), because we noticed that apart from dZN that represents the last layer\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# all of the other equations will use only A(i-1)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'A2'"
     ]
    }
   ],
   "source": [
    "deep_neural_network(X, y, learning_rate = 0.001, n_iter = 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
